---
title: "Statistical rethinking (mcelreath)"
---

<https://github.com/rmcelreath/stat_rethinking_2023>

```{r}
#| output: false
library(tidyverse)
library(magrittr)
library(rethinking)
library(triangle)
library(ggdag)
theme_set(theme_bw())
set.seed(2020)
```

Inference = a conclusion reached on the basis of evidence and reasoning.

Reasoning = thinking about something in a logical, sensible way.

"Researchers are entrusted with drawing inferences from the data".

Learning how to perform statistical inference is like learning to write a novel in a foreign language (statistics and probability theory).

## The cycle of science

[![](images/image-13316612.png)](https://bebi103a.github.io/lessons/01/cycle_of_science.html)

<https://bebi103a.github.io/lessons/01/cycle_of_science.html>

## Data analysis pipeline

[![](images/image-2023841330.png)](https://bebi103a.github.io/lessons/01/cycle_of_science.html)

<https://bebi103a.github.io/lessons/01/cycle_of_science.html>

## Bayes theorem

$$
P(A,B) = P(A \mid B) P(B)
$$

$$
P(B,A) = P(B \mid A) P(A)
$$

$$
P(A,B) = P(B,A)
$$

$$
P(A \mid B) P(B) = P(A,B) = P(B,A) = P(B \mid A) P(A)
$$

$$
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
$$

$$
P(H \mid D) = \frac{P(D \mid H) P(H)}{P(D)}
$$

$$
P(\theta \mid X) = \frac{P(X \mid \theta) P(\theta)}{P(X)}
$$

$$
P(\theta \mid X) = \frac{P(X \mid \theta) P(\theta)}{\int_\theta P(X \mid \theta) P(\theta) d\theta}
$$

$$
P(\theta \mid X) \propto P(X \mid \theta) P(\theta)
$$

$$
\text{posterior} \propto \text{likelihood} \times \text{prior}
$$

## Frequentist vs Bayesian

+------------------------------------------------+-------------------------------------------------------------+
| Frequentist                                    | Bayesian                                                    |
+================================================+=============================================================+
| Probability is "long-run relative frequency"   | Probability is "degree of certainty/relative plausability"  |
+------------------------------------------------+-------------------------------------------------------------+
| $P(X \mid \theta)$ is a sampling distribution\ | $P(X \mid \theta)$ is a likelihood function\                |
| (function of $X$ with $\theta$ fixed)          | (function of $\theta$ with $X$ fixed)                       |
+------------------------------------------------+-------------------------------------------------------------+
| No prior                                       | Prior                                                       |
+------------------------------------------------+-------------------------------------------------------------+
| P-values (NHST)                                | Full posterior distribution available for summary/decisions |
+------------------------------------------------+-------------------------------------------------------------+
| Confidence intervals                           | Credible intervals                                          |
+------------------------------------------------+-------------------------------------------------------------+
| Violates the "likelihood principle":           | Respects the "likelihood principle":                        |
|                                                |                                                             |
| -   Sampling intention matters                 | -   Sampling intention does not matter                      |
| -   Correction for multiple testing            | -   No correction for multiple testing                      |
| -   Adjustment for planned/post-hoc testing    | -   No adjustment for planned/post-hoc testing              |
+------------------------------------------------+-------------------------------------------------------------+
| Objective?                                     | Subjective?                                                 |
+------------------------------------------------+-------------------------------------------------------------+

## Bayesian inference worfklow

1.  Question/goal \[**estimand**\]

2.  Scientific model (of the data generation process, aka generative model)

3.  Statistical model(s) \[**estimator**\]

4.  Validate model

5.  Analyze data \[**estimate**\]

An **estimator** is a rule for calculating an **estimate** of a quantity of interest (the **estimand**) based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean (the estimand).

## Binomial estimation

### Question/goal/estimand

Globe tossing example. Estimate proportion of globe surface covered by water.

### Scientific model

```{r}
ggdag(dagify(
  WL ~ p,
  exposure = "p",
  outcome = "WL",
  labels = c(WL = "W,L", p = "pi")
)) + theme_dag_blank()
```

$W,L = f(\pi)$

### Statistical model

Observations (data) and explanations (parameters) are variables.

For each variable, must say how it is generated.

List variables as deterministic or stochastic/distributional functions:

$$
W=3,L=6 \sim \text{Binomial}(\pi)
$$

$$
\pi \sim \text{Uniform}(a=0, b=1)
$$

Stochastic/distributional functions can also be expressed as probability statements:

\[Data distribution/Likelihood function\]

$$
P(W = 3,L = 6 \mid \pi) =  \text{Binomial}(W = 3, L = 6, \pi) = \frac{(W+L)!}{W!L!} \pi^W (1 - \pi)^L = \\ \frac{(3+6)!}{3!6!} \pi^3 (1 - \pi)^6 = \frac{362880}{4320} \pi^3 (1 - \pi)^6  = 84 \pi^3 (1 - \pi)^6
$$

\[Prior distribution\]

$$
P(\pi \mid a=0,b=1) = \text{Uniform}(\pi, a=0, b=1) = \frac{1}{b - a} \\ = \frac{1}{1 - 0} = 1
$$

Bayesian estimator (posterior $\propto$ likelihood $\times$ prior):

$$
P(\pi \mid W = 3, L = 6) \propto \text{Binomial}(W = 3, L = 6, \pi) \times \text{Uniform}(\pi, a = 0, b = 1)
$$

$$
P(\pi \mid W = 3, L = 6) \propto 84 \pi^3 (1 - \pi)^6 \times 1
$$

$$
P(\pi \mid W = 3, L = 6) \propto \pi^3 (1 - \pi)^6
$$

```{r}
WL = c("W", "W", "L", "L", "L", "L", "W", "L", "L")

W = sum(WL == "W")
L = sum(WL == "L")

#W = 10
#L = 20

p = seq(from = 0, to = 1, len = 1000)

a = 0
b = 1

binomial <- function(W, L, p) {
  return(factorial(W + L)/(factorial(W)*factorial(L)) * p^W * (1 - p)^L)
}

uniform <- function(p, a, b) {
  return(1/(b - a))
}

likelihood = binomial(W, L, p)
prior = uniform(p, a, b)

#likelihood = dbinom(W, W+L, p)
#prior = dunif(p, a, b)

posterior = likelihood * prior / sum(likelihood)

plot(posterior ~ p)
```

### Validate model

### Analyze data

## Linear regression

1.  Question/goal \[estimand\]: State a clear **question** (inferential or descriptive)

    -   Describe association between (adult) weight and height

2.  Scientific model: Sketch the causal **assumptions** (DAG)

    -   Height → Weight ← \[Unobserved\]

    -   Weight is some function of height \[and other unobserved variables/influences\] $W = f(H,U)$

3.  Statistical model: Use the sketch to define a **generative model**

    -   What is this function $f$ ?

    -   How does height influences weight?

    -   For example, a linear function: $W = \alpha + \beta H + U$

    -   It should be able to generate realistic data!

4.  Statistical model: Use the generative model to build an **estimator**

    -   We want to estimate how the average weight changes with height.

    -   How does average weight changes with height?

    -   $E(W_i \mid H_i) = \alpha + \beta H_i$

    -   In Bayesian inference the estimator is always the posterior distribution.

    -   $P(\alpha, \beta, \sigma \mid H_i, W_i) = \frac{P(W_i \mid H_i, \alpha, \beta, \sigma) P(\alpha, \beta, \sigma)}{Z}$

    -   In conventional statistical model notation:

        $$
        W_i \sim \text{Normal}(\mu_i, \sigma) \\
        \mu_i = \alpha + \beta H_i \\
        \alpha \sim \text{Normal}(0, 10) \\
        \beta \sim \text{Uniform}(0, 1) \\
        \sigma \sim \text{Uniform}(0, 10) \\
        $$

        $W$ is distributed normally with mean that is a linear function of $H$.

    -   Prior predictive distribution.

5.  Validate statistical model

    -   Analyze data simulated using the generative model.

6.  Use the statistical model to analyze data/calculate estimate

    -   Estimate = Posterior distribution.

    -   Posterior predictive distribution.

An **estimator** is a rule for calculating an **estimate** of a quantity of interest (the **estimand**) based on observed data: thus the rule (the estimator), the quantity of interest (the estimand) and its result (the estimate) are distinguished. For example, the sample mean is a commonly used estimator of the population mean (the estimand).

### Generative model

```{r}
sim_weight <- function(H, beta, sd) {
  U = rnorm(length(H), 0, sd)
  W = beta * H + U
  return(W)
}

n = 100
H = runif(n, min = 130, max = 170)
W = sim_weight(H, beta = 0.5, sd = 5)
plot(W ~ H, col = "red")
```

### Conventional statistical model notation

1.  List the variables

2.  Define each variable as a deterministic or distributional/stochastic function of the other variables

$$
W_i = \beta H_i + U_i \\
U_i \sim \text{Normal}(0, \sigma) \\
H_i \sim \text{Uniform}(130, 170)
$$

Variables on the left, functions on the right (equal sign for deterministic functions, tilde sign for distributional/stochastic functions).

### Prior predictive distribution

```{r}
n = 1000
a = rnorm(n, 0, 10)
b = runif(n, 0, 1)
plot(NULL,
     xlim = c(130, 170),
     ylim = c(55,95),
     xlab = "height (cm)",
     ylab = "weight (kg)")
for(j in 1:50) abline(a = a[j], b = b[j], col = "red")
```

```{r}
n = 10
H = runif(n, min = 130, max = 170)
W = sim_weight(H, beta = 0.5, sd = 5)

m3.1 = quap(
  alist(
    W ~ dnorm(mu, sigma),
    mu <- a + b * H,
    a ~ dnorm(0, 10),
    b ~ dunif(0, 1),
    sigma ~ dunif(0, 10)
  ), data = tibble(W, H))

precis(m3.1)
```
