---
title: "DataCamp: Fundamentals of BDA in R"
format:
  html:
    df-print: paged
    toc: true
---

```{r}
#| output: false
library(tidyverse)
library(mosaic)
library(ggformula)
library(ggside)
library(ggjoy)
library(broom)
library(bayesplot)
library(bayestestR)
library(patchwork)
library(rethinking)

theme_set(theme_bw())

set.seed(123456)
```

## What is Bayesian data analysis?

This chapter will introduce you to Bayesian data analysis and give you a feel for how it works.

### Bayesian/probabilistic inference in a nutshell

A method for figuring out unobservable quantities given known facts that uses probability to describe the uncertainty over what the values of the unknown quantities could be

### Bayesian data analysis

-   The use of Bayesian inference to learn (parameter estimates, future data, etc) from data

-   It can be used for hypothesis testing, linear regression, etc.

-   It is very flexible and allows the data analyst to construct problem-specific models

### Probability

-   A number between 0 and 1

-   A statement about certainty/uncertainty

-   1 is complete certainty something is the case

-   0 is complete certainty something is NOT the case

-   Not only about yes/no events (see continuous probability distribution below)

    ![](images/clipboard-2142904166.png){width="278"}

> The role of probability distributions in BDA is to represent uncertainty, and the role of Bayesian inference is to update probability distributions to reflect what has been learned from data

### A Bayesian model for the proportion of success

The function `prop_model` implements a Bayesian model that assumes that:

-   The `data` is a vector of successes and failures represented by `1`s and `0`s

-   There is an unknown underlying proportion of success

-   Prior to being updated with data any underlying proportion of success is equally likely

The result is a probability distribution that represents what the model knows about the underlying proportion of success

```{r}
prop_model <- function (data = c(), prior_prop = c(1, 1), n_draws = 10000, 
    show_plot = TRUE) 
{
    data <- as.logical(data)
    proportion_success <- c(0, seq(0, 1, length.out = 100), 1)
    data_indices <- round(seq(0, length(data), length.out = min(length(data) + 
        1, 20)))
    post_curves <- map_dfr(data_indices, function(i) {
        value <- ifelse(i == 0, "Prior", ifelse(data[i], "Success", 
            "Failure"))
        label <- paste0("n=", i)
        probability <- dbeta(proportion_success, prior_prop[1] + 
            sum(data[seq_len(i)]), prior_prop[2] + sum(!data[seq_len(i)]))
        probability <- probability/max(probability)
        data_frame(value, label, proportion_success, probability)
    })
    post_curves$label <- fct_rev(factor(post_curves$label, levels = paste0("n=", 
        data_indices)))
    post_curves$value <- factor(post_curves$value, levels = c("Prior", 
        "Success", "Failure"))
    p <- ggplot(post_curves, aes(x = proportion_success, y = label, 
        height = probability, fill = value)) + geom_joy(stat = "identity", 
        color = "white", alpha = 0.8, panel_scaling = TRUE, linewidth = 1) + 
        scale_y_discrete("", expand = c(0.01, 0)) + scale_x_continuous("Underlying proportion of success") + 
        scale_fill_manual(values = hcl(120 * 2:0 + 15, 100, 65), 
            name = "", drop = FALSE, labels = c("Prior   ", "Success   ", 
                "Failure   ")) + theme_light(base_size = 18) + 
        theme(legend.position = "top")
    if (show_plot) {
        print(p)
    }
    invisible(rbeta(n_draws, prior_prop[1] + sum(data), prior_prop[2] + 
        sum(!data)))
}
```

```{r}
data <- c()

prop_model(data)
```

```{r}
data <- c(0)

prop_model(data)
```

```{r}
data <- c(0, 1)

prop_model(data)
```

```{r}
data <- c(0, 1, 0, 0, 0)

prop_model(data)
```

```{r}
data <- c(0, 1, 0, 0, 0, 1)

prop_model(data)
```

### Zombie cure

![](images/il_1080xN.736851073_n5ij.avif)

Let's say the zombie apocalypse is upon us and we have come up with a new experimental drug to cure zombieism. We have no clue how effective it's going to be, but when we gave it to 13 zombies two of them turned human again.

QOI = proportion of zombies cured by drug

The model implemented in `prop_model` makes more sense here than in the fair coin tossing experiment as we have no clue how good the drug is. The final probability distribution (with `x = 2` successes/zombie cured out of `n = 13` zombies who were given the drug) represents what the model now knows about the underlying proportion of cured zombies.

What proportion of zombies would we expect to turn human if we administered this new drug to the whole zombie population?

Between 5% to 40%.

```{r}
#| fig-height: 8
data = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)

prop_model(data)
```

### Priors and posteriors

-   A **prior** is a probability distribution that represents what the model knows before seeing the data (the blue distribution above)

-   A **posterior** is a probability distribution that represents what the model knows after having seen the data (the last red distribution above)

A probability distribution can be represented by a plot, a mathematical function (if it exists), or a vector of samples where a value occurs proportionally often to how probable it is.

![](images/clipboard-3749826059.png)

```{r}
# number of dice
num_dice <- 5

# number of sides on each die
num_sides <- 6

# simulate rolling five dice and counting the number of 6's
number_of_sixes <- replicate(10000, sum(sample(1:num_sides, num_dice, replace = TRUE) == 6))

number_of_sixes[1:20]
```

```{r}
gf_bar(~ number_of_sixes, fill = "skyblue")
```

### Summarizing and reporting the results of the zombie drug experiment

In addition to showing the plot, `prop_model` also returns a large (N = 10,000) random sample from the posterior over the underlying proportion of success.

```{r}
data = c(1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0)
         
# extract and explore the posterior
posterior = prop_model(data, show_plot = FALSE)

posterior[1:20]
```

The point of working with samples from a probability distribution is that it makes it *easy* to calculate new quantities of interest.

-   Take a look at the distribution of *all* the samples in `posterior` by plotting it as a histogram or density plot.

```{r}
gf_density(~ posterior, fill = "skyblue")
```

A *point estimate* is a single number used to summarize what's known about a parameter of interest. It can be seen as a "best guess" of the value of the parameter. A commonly used point estimate is the **median** of the posterior. It's the midpoint of the distribution, and it's equally probable for the parameter value to be larger than the median as it is to be smaller than it.

```{r}
mean(posterior)
median(posterior)
```

So, a best guess is that the drug would cure around 18% of all zombies. Another common summary is to report an interval that includes the parameter of interest with a certain probability. This is called a *credible interval* (CI). With a posterior represented as a vector of samples you can calculate a CI using the `quantile()` function.

`quantile()` takes the vector of samples as its first argument and the second argument is a vector defining how much probability should be left below and above the CI. For example, the vector `c(0.05, 0.95)` would yield a 90% CI and `c(0.25, 0.75)` would yield a 50% CI.

-   Calculate a 90% credible interval of `posterior` using `quantile()`.

```{r}
quantile(posterior, c(0.05, 0.95))
```

```{r}
favstats(posterior)
```

```{r}
hdi(posterior, ci = .89)
```

According to the credible interval, there is a 90% probability that the proportion of zombies the drug would cure is between 6% and 38%. (Here we have to be careful to remember that the percentage of cured zombies and the percentage of probability are two different things.)

Now, there is a rival zombie laboratory that is also working on a drug. They claim that they are certain that their drug cures 7% of the zombies it's administered to. Can we calculate how probable it is that our drug is better? Yes, we can! But it's a two stage process:

1.  Use `sum` to count how many samples in `posterior` that are larger than 7%

```{r}
sum(posterior > 0.07)
```

To turn this count into a probability we now need to *normalize* it, that is, divide it by the total number of samples in `posterior`.

2.  Divide the result of `sum` by the number of samples in `posterior`

```{r}
sum(posterior > 0.07)/length(posterior)
```

or, in one step:

```{r}
mean(posterior > 0.07)
```

> Given the data of two cured and 11 relapsed zombies, and using the Bayesian model described before, there is a 90% probability that our drug cures between 6% and 39% of treated zombies. Further, there is 93% probability that our drug cures zombies at a higher rate than current state of the art drugs.

Sounds like science to me!

## How does Bayesian inference work?

-   Bayesian inference requires three ingredients:

    1.  Data

    2.  Generative model

    3.  Priors

![](images/clipboard-4290210799.png)

### Generative model of the zombie drug experiment

```{r}
# parameters
prop_success <- 0.18
n_zombies <- 13

# simulate random samples/data
zombies_cured <- map_int(1:n_zombies, \(x) runif(1, min = 0, max = 1) < prop_success)

zombies_cured
```

```{r}
# count zombie cured
zombies_cured <- sum(zombies_cured)

zombies_cured
```

It turns out that this generative model already has a name. It's called the **binomial process** or the **binomial distribution**. In R you can use the `rbinom` function to simulate data from a binomial distribution. The `rbinom` function takes three arguments:

-   `n` The number of times you want to run the generative model

-   `size` The number of trials. (For example, the number of zombies you're giving the drug.)

-   `prob` The underlying proportion of success as a number between `0.0` and `1.0`.

A nice thing with `rbinom` is that it makes it easy to rerun the generative model many times.

```{r}
n_samples <- 10000

zombies_cured = rbinom(n = n_samples, size = n_zombies, prob = prop_success)

zombies_cured[1:20]
```

It’s actually the case that all probability distributions, like the normal distribution or the Poisson distribution, can be seen as small generative models, and there are many more that are implemented in R.

### Using a generative model

With a generative model, we can plugin fixed parameter values and it will generate simulated data. This is useful if we know the parameter values, and we want to predict what future unknown data might be.

![](images/clipboard-1705872009.png)

For example, say we are completely sure that our drug cures 10% of all zombies, and we want to know how many we'll likely cure when we give the drug to the 100 zombies we have in our zombie pit.

![](images/clipboard-2700806159.png)

```{r}
zombies_cured <- rbinom(n = 10000, size = 100, prob = 0.1) # sampling distribution

zombies_cured[1:20]
```

```{r}
gf_bar(~ zombies_cured, fill = "skyblue")
```

From known parameters (of the DGP) to unknown data (sampling distribution):

![](images/clipboard-2619356156.png)

But we usually want to go from known data (observed data) to unknown parameters (of the DGP):

![](images/clipboard-2132806111.png)

That's what we use Bayesian inference for! But for that we need data and priors

![](images/clipboard-396486842.png)

### Representing uncertainty with priors and adding a prior to the model

Now, let's assume our drug cures \~10% of all zombies, but we are not completely sure.

![](images/clipboard-3729084674.png)

We could represent our uncertainty as a uniform distribution from 0 to 20% and instead of assigning `proportion_clicks` a single value of 10%, we are going to assign it a large number of values drawn from a probability distribution.

```{r}
n_samples <- 100000

# parameters
prop_success <- runif(n_samples, min = 0, max = 0.2)
n_zombies <- 100

# simulate data (combining uncertainty in prop_success with uncertainty in data)
zombies_cured = rbinom(n = n_samples, size = n_zombies, prob = prop_success)
```

Because the `rbinom` function is *vectorized* the first value of `prop_success` is used to sample the first value in `zombies_cured`, the second value in `prop_success` is used for the second in `zombies_cured`, and so on. The result is that the samples in `zombies_cured` now also incorporate the uncertainty in what the underlying proportion of success could be.

```{r}
gf_density(~ prop_success, fill = "skyblue")
```

```{r}
gf_bar(~ zombies_cured, fill = "skyblue")
```

This looks very different from the histogram of `zombies_cured` we got in the last exercise when `prop_success` was exactly 0.1. With the added uncertainty in `prop_success` the uncertainty over the number of cured zombies has also increased.

### Bayesian models and conditioning (update a Bayesian model with data)

Bayesian model = generative model + priors

![](images/clipboard-355160551.png)

![![](images/clipboard-3830224153.png)](images/clipboard-2168423729.png)

We observe that 13 out of 100 zombies we have given the drug to were cured.

![](images/zombie_cure.jpg)

```{r}
n_samples <- 100000

# parameters
prop_success <- runif(n_samples, min = 0, max = 0.2)
n_zombies <- 100

# simulate random samples/data (combining uncertainty in prop_success with uncertainty in data)
zombies_cured = rbinom(n = n_samples, size = n_zombies, prob = prop_success)
```

```{r}
# joint probability distribution of parameter values and simulated random samples/data
# before seeing the data
prior <- tibble(prop_success, zombies_cured)

prior
```

```{r}
gf_point(prop_success ~ zombies_cured, color = "skyblue", alpha = 0.1, data = prior) |> 
  gf_density_2d() +
  geom_ysidedensity(color = "skyblue") +
  geom_xsidebar(fill = "skyblue")
```

![If we knew the exact value of `prop_success` we could simply condition our prior distribution using that value (e.g., 10% or 5 or 15%):](images/clipboard-1100320571.png)

![](images/clipboard-693681996.png)

![![](images/clipboard-250264265.png)](images/clipboard-825126670.png)

But can also do conditioning the other way around using the observed data (e.g., 5, 10, 15, 20, or 25 cured zombies):

![](images/clipboard-801822657.png)

![](images/clipboard-137729353.png)

![](images/clipboard-2263246739.png)

![](images/clipboard-2878087164.png)

![](images/clipboard-2009918851.png)

### The essence of Bayesian inference

> Bayesian inference is conditioning on data, in order to learn about parameter values.

### Update a Bayesian model with data

The reason we've called it `prior` is because it represents the uncertainty *before* (that is, *prior* to) having included the information in the data. Let's do that now!

You ran your clinical trial of the zombie drug, and 13 out of the 100 zombies in the trial were cured. You would now like to use this new information to update the Bayesian model.

`prior$zombies_cured` represented the uncertainty over how many zombies you would cure because of the drug. But now you know you got exactly 13 cured zombies.

-   Update `prior` to include this information by conditioning on this data. That is, filtering away all rows where `prior$zombies_cured` isn't equal to 13. Store the resulting data frame in `posterior`.

The reason that we call it `posterior` is because it represents the uncertainty *after* (that is, *posterior* to) having included the information in the data.

```{r}
posterior <- prior |> filter(zombies_cured == 13) # or `prior[prior$zombies_cured == 13, ]`

posterior
```

```{r}
gf_density(~ prop_success, fill = "skyblue", data = posterior)
```

This doesn't look at all like the uniform distribution between 0.0 and 0.2 we put into `prop_success` before. The whole distribution of samples now represent the posterior (after the data) probability distribution over what `prop_success` could be.

Now we want to use this updated `prop_success` to predict how many zombies we would cure if we reran the clinical trial.

While `prop_success` represents the uncertainty regarding the underlying proportion of clicks for the next ad campaign, `zombies_cured` has not been updated yet.

-   Replace `prior$zombies_cured` with a new sample drawn using `rbinom` with `prior$prop_success` as an argument.

```{r}
# assign posterior to a new variable called prior
prior <- posterior

# replace prior$zombies_cured with a new sample and visualize the result
n_samples <- nrow(prior)
n_zombies <- 100

prior$zombies_cured <- rbinom(n_samples, size = n_zombies, prob = prior$prop_success)
```

-   Plot the resulting `prior$zombies_cured`

```{r}
gf_bar(~ zombies_cured, data = prior)
```

The plot shows a probability distribution over the number of zombies that would be cured in a new clinical trial.

-   Calculate the probability that you will get 5 or more zombies cured next time you run your clinical trial.

```{r}
prior |> summarize(`P(zombies_cured >= 5)` = mean(zombies_cured >= 5)) # or `mean(prior$zombies_cured >= 5)`
```

### What have we done?

1\) Statistical model = Data/sampling probability of `zombies_cured`, probability model of the DGP (`dbinom`, binomial process)

![](images/clipboard-2236362687.png)

2\) Prior probability distribution of `prop_success` (`dunif`, uniform between 0 and 0.2)

![](images/clipboard-2701134919.png)

3\) Joint probability distribution of `zombies_cured` and `prop_success`

![](images/clipboard-4090737218.png)

4\) Collect data and condition the joint probability distribution on the data

![](images/clipboard-780549006.png)

5\) to obtain the posterior distribution (Bayesian updating)

![](images/clipboard-4259084839.png)

6\) Use posterior as prior for the next BDA

![To recap, we have:](images/clipboard-846239201.png)

1.  Specified prior information\
    ![](images/clipboard-2433292234.png)
2.  and a generative model\
    ![](images/clipboard-1549079272.png)
3.  and given some data,\
    ![](images/clipboard-3290617798.png)
4.  we calculated the posterior/learned about the DGP parameter values (with uncertainty)\
    ![](images/clipboard-3577203425.png)
5.  this works for any generative model with any number of parameters\
    ![](images/clipboard-2068686318.png)\

![](images/clipboard-4237198782.png)

## Why use Bayesian data analysis?

BDA is flexible!

-   You can include information sources in addition to data
-   You can make any comparisons between groups or data sets
-   You can use the results of BDA to do Decision Analysis
-   You can change the underlying statistical model

### Change the model to use an informative prior

Including information in addition to data:

-   Background information
-   Expert opinion
-   Common knowledge

![](images/clipboard-3881761330.png)

![](images/clipboard-2489642209.png)

The Beta distribution is a useful probability distribution when you want model uncertainty over a parameter bounded between 0 and 1. Here you'll explore how the two parameters of the Beta distribution determine its shape.

![](images/clipboard-1156946050.png)

A Beta(1,1) distribution is the same as a uniform distribution between 0 and 1. It is useful as a so-called *non-informative* prior as it expresses than any value from 0 to 1 is equally likely.

![](images/clipboard-3817040415.png)

![](images/clipboard-3431059699.png)

So the larger the shape parameters are, the more concentrated the beta distribution becomes.

![](images/clipboard-4275362894.png)

![](images/clipboard-919083096.png)

![](images/clipboard-1477066393.png)

![](images/clipboard-527699540.png)

The new information you got from the zombie drug manufacturer was:

> Most zombies are cured on 5% of the time, but for some zombies it is as low as 2% and for others as high as 8%.

![`rbeta(shape1 = 5, shape2 = 95)` (C) is a reasonable choice of prior distribution for the above information!](images/clipboard-355123039.png)

```{r}
n_samples <- 100000

# parameters
prop_success <- rbeta(n_samples, shape1 = 5, shape2 = 95)
n_zombies <- 100

# simulate data (combining uncertainty in prop_success with uncertainty in data)
zombies_cured = rbinom(n = n_samples, size = n_zombies, prob = prop_success)

prior <- tibble(prop_success, zombies_cured)

posterior <- prior[prior$zombies_cured == 13, ]

gf_density(~ prop_success, fill = "skyblue", data = prior) / gf_density(~ prop_success, fill = "skyblue", data = posterior)
```

Two different Bayesian models for the same data:

![](images/clipboard-589544108.png)

### Contrasts and comparisons

Comparing drug A (video) vs. drug B (text) \[same model but different data\]

![](images/clipboard-1514466049.png)

```{r}
n_samples <- 100000

# parameters
prop_success <- runif(n_samples, min = 0, max = 0.2)
n_zombies <- 100

# simulate data (combining uncertainty in prop_success with uncertainty in data)
zombies_cured = rbinom(n = n_samples, size = n_zombies, prob = prop_success)


prior <- tibble(prop_success, zombies_cured)

posterior_video = prior[prior$zombies_cured == 13, ]
posterior_text = prior[prior$zombies_cured == 6, ]

gf_density(~ prop_success, fill = "skyblue", data = posterior_video) / gf_density(~ prop_success, fill = "skyblue", data = posterior_text)
```

Calculating the posterior difference \[as an example of a probability distribution over an interesting parameter or function of parameter(s)\]:

The posterior `prop_success` for the video and text ad has been put into a single `posterior` data frame. The reason for `[1:4000]` is because these `prop_success`s are not necessarily of the same length, which they need to be when put into a data frame.

-   Calculate the posterior probability distribution over what the difference in proportion of zombies cured might be between drug A (video ad) and drug B (text ad).

```{r}
posterior <- tibble(
    video_prop = posterior_video$prop_success[1:4000],
    text_prop  = posterior_text$prop_success[1:4000])
    
# calculate the posterior difference: `video_prop - text_prop`
posterior$prop_diff = posterior$video_prop - posterior$text_prop

# visualize `prop_diff`
gf_density(~ prop_diff, fill = "skyblue", data = posterior)
```

alternatively:

```{r}
n_samples <- min(nrow(posterior_video), nrow(posterior_text))

posterior <- tibble(
    video_prop = posterior_video |> sample_n(n_samples) |> pull(prop_success),
    text_prop  = posterior_text |> sample_n(n_samples) |> pull(prop_success))
    
# calculate the posterior difference: `video_prop - text_prop`
posterior$prop_diff = posterior$video_prop - posterior$text_prop

# visualize `prop_diff`
gf_density(~ prop_diff, fill = "skyblue", data = posterior)
```

-   Calculate "a most likely" difference by taking the `median` of `posterior$prop_diff`

```{r}
favstats(~ prop_diff, data = posterior)
```

-   Calculate the probability that proportion of success is larger for drug A (video ad) than for drug B (text ad). That is, calculate the proportion of samples in `posterior$prop_diff` that are more than zero.

```{r}
posterior |> summarize(`P(drug A better than drug B)` = mean(prop_diff > 0))
```

-   Finally, calculate the 95% (or 89%) HDPI:

```{r}
hdi(posterior$prop_diff)

hdi(posterior$prop_diff, ci = .89)
```

### Decision analysis

```{r}
video_cost <- 0.25 # money spent making drug A/video
text_cost <- 0.05 # money spent making drug B/text
zombie_profit <- 2.5 # money saved curing zombies rather than killing them

posterior <- posterior |>
  mutate(video_profit = video_prop * zombie_profit - video_cost,
         text_profit = text_prop * zombie_profit - text_cost,
         profit_diff = video_profit - text_profit)

posterior
```

Make a data-informed decision based on cost-profit calculations done above:

```{r}
gf_density(~ video_profit, fill = "skyblue", data = posterior) |>
  gf_lims(x = c(-0.5, 0.5)) /
  gf_density(~ text_profit, fill = "skyblue", data = posterior) |>
  gf_lims(x = c(-0.5, 0.5)) /
  gf_density(~ profit_diff, fill = "skyblue", data = posterior) |>
  gf_lims(x = c(-0.5, 0.5))
```

```{r}
favstats(~ profit_diff, data = posterior)
```

```{r}
posterior |> summarize(`P(drug A/video more profitable than drug B/text)` = mean(profit_diff > 0)) 
```

So it seems that the evidence does not strongly favor neither drug A/video nor drug B/text, but if forced to choose at this point, what would you choose?

Drug B/text (1 - 0.36 = 64% probability of being more profitable than drug A)

![](images/clipboard-3468439359.png)

### Change anything and everything

Completely switch out the binomial model with a Poisson model (with parameter λ = rate of zombies cured per day).

The Poisson distribution simulates a process where the outcome is a number of occurrences per day/year/area/unit/etc.

The Poisson distribution has one parameter (λ), the average number of events per unit.

```{r}
zombies_cured_per_day <- rpois(n = n_samples, lambda = 20)

gf_bar(~ zombies_cured_per_day)
```

```{r}
n_samples <- 100000

# parameters
rate_success <- runif(n_samples, min = 0, max = 80)

# simulate data (combining uncertainty in rate_success with uncertainty in data)
zombies_cured_per_day = rpois(n = n_samples, lambda = rate_success)

# joint probability distribution of parameter values and simulated random samples/data
# before seeing the data
prior <- tibble(rate_success, zombies_cured_per_day)

prior

posterior <- prior[prior$zombies_cured_per_day == 19, ]

gf_density(~ rate_success, fill = "skyblue", data = prior) / gf_density(~ rate_success, fill = "skyblue", data = posterior)
```

```{r}
hdi(posterior$rate_success)
```

![](images/clipboard-3487640598.png)

### Bayesian inference is optimal, kind of…

Optimal only in the small world of the model, not in the big world of reality!

## How to fit Bayesian models more efficiently

### Probability, conditional probability, and probability distributions

-   P(`zombies_cured` = 13) is a probability

-   P(`zombies_cured`) is a probability *distribution*

-   P(`zombies_cured` = 13 \| `prop_success` = 10%) is a *conditional* probability

-   P(`zombies_cured` \| `prop_success` = 10%) is a *conditional* probability *distribution*

### Manipulating probability: The rules of probability

-   The sum rule:

$$
p(A \lor B) = p(A) + p(B) - p(A \land B)
$$

-   The product rule:

$$
p(A \land B) = p(A) \cdot p(B \mid A)
$$

### Calculating likelihoods

So we can calculate new probabilities using the sum and the product rules. But earlier we didn't calculate probabilities directly, we simulated.

If we wanted to figure out the probability that a generative model would generate a certain value, we simulated a large number of samples and then counted the proportion of samples taking on this certain value. This was easy to do for generative models that correspond to common probability distributions, like the binomial or the Poisson distribution, as you can use the “r”-functions, like `rbinom` and `rpois` for this.

But one thing that faster Bayesian computational methods have in common is that they require that this probability can be calculated directly rather than simulated. Fortunately, for these common distributions someone already figured out how to do this and in R we can use the “d”-functions, like `dbinom` and `dpois`, for this.

This is obviously much more efficient than to have to generate 100,000 or so samples first. We can manipulate the resulting probabilities using the rules we just learned.

Finally, if we want to get a whole probability distribution, we’ll have to calculate the probability for a range of values. But the d-functions are generally vectorized, so that is easy!

![](images/clipboard-1758485739.png)

![](images/clipboard-3097915879.png)

![](images/clipboard-2503161844.png)

![](images/clipboard-3553114941.png)

![](images/clipboard-1550659980.png)

-   Simulate P(`zombies_cured` = 13 \| `prop_success` = 10%)

```{r}
zombies_cured <- rbinom(n = n_samples, size = n_zombies, prob = 0.1)
mean(zombies_cured == 13)
```

-   Calculate P(`zombies_cured` = 13 \| `prop_success` = 10%)

```{r}
dbinom(13, size = n_zombies, prob = 0.1)
```

-   Calculate P(`zombies_cured` = 13 OR `zombies_cured` = 14 \| `prop_success` = 10%)

```{r}
dbinom(13, size = n_zombies, prob = 0.1) + dbinom(14, size = n_zombies, prob = 0.1)
```

-   Calculate P(`zombies_cured` \| `prop_success` = 10%)

```{r}
zombies_cured <- 1:n_zombies

p_zombies_cured <- dbinom(zombies_cured, size = n_zombies, prob = 0.1) # probability distribution

# plot(zombies_cured, p_zombies_cured, type = "h")

gf_point(p_zombies_cured ~ zombies_cured) |> gf_segment(x = ~zombies_cured, xend = ~zombies_cured, y = 0, yend = ~p_zombies_cured)
```

### Bayesian inference by calculation

```{r}
#| warning: false
prop_success <- seq(0, 1, by = 0.01)
n_zombies <- 100
zombies_cured <- 1:n_zombies

d <- expand_grid(prop_success, zombies_cured)

d <- d |> mutate(prior = dunif(prop_success, min = 0, max = 0.2),
                 likelihood = dbinom(zombies_cured, size = n_zombies, prob = prop_success),
                 posterior = prior * likelihood / sum(prior * likelihood))
         
gf_raster(posterior ~ zombies_cured + prop_success, data = d) |> gf_lims(x = c(0, 50), y = c(0, 0.2)) |> gf_refine(scale_fill_gradientn(colours=c("white", "yellow","red")))
```

```{r}
#| warning: false
d <- d |>
  filter(zombies_cured == 13) |> 
  mutate(posterior = posterior / sum(posterior))

gf_raster(posterior ~ zombies_cured + prop_success, data = d) |> gf_lims(x = c(0, 50), y = c(0, 0.2)) |> gf_refine(scale_fill_gradientn(colours=c("white", "yellow","red")))
```

```{r}
#| warning: false
gf_point(posterior ~ prop_success, data = d) |> gf_segment(x = ~prop_success, xend = ~prop_success, y = 0, yend = ~posterior) |> gf_lims(x = c(0, 0.25))
```

![](images/clipboard-742346553.png)

### A conditional shortcut

You've now done some Bayesian computation, without doing any simulation! The plot you produced should be similar to the posterior distribution you calculated with simulation. However, it required an awful lot of code, isn't there anything we can cut?

Yes, there is! You can directly condition on the data, no need to first create the joint distribution.

```{r}
#| warning: false
prop_success <- seq(0, 1, by = 0.01)
n_zombies <- 100
zombies_cured <- 13

d <- tibble(prior = dunif(prop_success, min = 0, max = 0.2),
            likelihood = dbinom(zombies_cured, size = n_zombies, prob = prop_success),
            posterior = prior * likelihood / sum(prior * likelihood))
         
gf_raster(posterior ~ zombies_cured + prop_success, data = d) |> gf_lims(x = c(0, 50), y = c(0, 0.2)) |> gf_refine(scale_fill_gradientn(colours=c("white", "yellow","red")))
```

```{r}
#| warning: false
gf_point(posterior ~ prop_success, data = d) |> gf_segment(x = ~prop_success, xend = ~prop_success, y = 0, yend = ~posterior) |> gf_lims(x = c(0, 0.25))
```

### Bayes' theorem

```{r}
#| eval: false
posterior = prior * likelihood / sum(posterior)
```

![![](images/clipboard-1594329491.png)](images/clipboard-2636086849.png)

### Grid approximation

-   Define a grid over all the parameter combinations you need to evaluate

-   Approximate as it's often impossible to try all parameter combinations

-   There are many more algorithms to fit a Bayesian model, some more efficient that others

### A mathematical notation for Bayesian models

$$
x \sim \text{Binom}(n, \pi) \\
n = 100 \\
\pi \sim \text{Unif}(0.0, 0.2)
$$

```{r}
m1 <- quap(
  alist(
    x ~ dbinom(n, π),
    n <- 100,
    π ~ dunif(0, 0.2)
  ),
  data=list(x=13))

summary(m1)
```

## More parameters, more data!
