<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.475">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Edoardo “Dado” Marcora">
<meta name="dcterms.date" content="2023-04-07">

<title>Statistical inference</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="king2_files/libs/clipboard/clipboard.min.js"></script>
<script src="king2_files/libs/quarto-html/quarto.js"></script>
<script src="king2_files/libs/quarto-html/popper.min.js"></script>
<script src="king2_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="king2_files/libs/quarto-html/anchor.min.js"></script>
<link href="king2_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="king2_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="king2_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="king2_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="king2_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body>

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#reasoning-under-uncertainty" id="toc-reasoning-under-uncertainty" class="nav-link active" data-scroll-target="#reasoning-under-uncertainty">Reasoning under uncertainty</a>
  <ul class="collapse">
  <li><a href="#axioms-of-probability-theory" id="toc-axioms-of-probability-theory" class="nav-link" data-scroll-target="#axioms-of-probability-theory">Axioms of probability theory</a></li>
  <li><a href="#theorems-a.k.a.-rules-of-probability-theory" id="toc-theorems-a.k.a.-rules-of-probability-theory" class="nav-link" data-scroll-target="#theorems-a.k.a.-rules-of-probability-theory">Theorems (a.k.a. rules) of probability theory</a></li>
  <li><a href="#definitions-of-probability-theory" id="toc-definitions-of-probability-theory" class="nav-link" data-scroll-target="#definitions-of-probability-theory">Definitions of probability theory</a></li>
  <li><a href="#bayes-theoremrule" id="toc-bayes-theoremrule" class="nav-link" data-scroll-target="#bayes-theoremrule">Bayes’ theorem/rule</a></li>
  <li><a href="#frequentist-vs-bayesian-inference" id="toc-frequentist-vs-bayesian-inference" class="nav-link" data-scroll-target="#frequentist-vs-bayesian-inference">Frequentist vs Bayesian inference</a></li>
  </ul></li>
  <li><a href="#quantities-of-interest-in-empirical-quantitative-research" id="toc-quantities-of-interest-in-empirical-quantitative-research" class="nav-link" data-scroll-target="#quantities-of-interest-in-empirical-quantitative-research">Quantities of interest in empirical quantitative research</a></li>
  <li><a href="#statistical-inference" id="toc-statistical-inference" class="nav-link" data-scroll-target="#statistical-inference">Statistical inference</a></li>
  <li><a href="#statistical-models" id="toc-statistical-models" class="nav-link" data-scroll-target="#statistical-models">Statistical models</a>
  <ul class="collapse">
  <li><a href="#notation" id="toc-notation" class="nav-link" data-scroll-target="#notation">Notation</a></li>
  <li><a href="#equivalent-linear-regression-notation" id="toc-equivalent-linear-regression-notation" class="nav-link" data-scroll-target="#equivalent-linear-regression-notation">Equivalent linear regression notation</a></li>
  <li><a href="#forms-of-uncertainty" id="toc-forms-of-uncertainty" class="nav-link" data-scroll-target="#forms-of-uncertainty">Forms of uncertainty</a></li>
  <li><a href="#examples-of-systematic-components" id="toc-examples-of-systematic-components" class="nav-link" data-scroll-target="#examples-of-systematic-components">Examples of systematic components</a></li>
  <li><a href="#examples-of-stochastic-components" id="toc-examples-of-stochastic-components" class="nav-link" data-scroll-target="#examples-of-stochastic-components">Examples of stochastic components</a></li>
  <li><a href="#choosing-systematic-and-stochastic-components" id="toc-choosing-systematic-and-stochastic-components" class="nav-link" data-scroll-target="#choosing-systematic-and-stochastic-components">Choosing systematic and stochastic components</a></li>
  </ul></li>
  <li><a href="#data-generation-processes-with-simulation" id="toc-data-generation-processes-with-simulation" class="nav-link" data-scroll-target="#data-generation-processes-with-simulation">Data generation processes (with simulation)</a></li>
  <li><a href="#probability" id="toc-probability" class="nav-link" data-scroll-target="#probability">Probability</a>
  <ul class="collapse">
  <li><a href="#probability-is-a-function" id="toc-probability-is-a-function" class="nav-link" data-scroll-target="#probability-is-a-function">Probability is a function</a></li>
  <li><a href="#probability-distributions-are-also-functions" id="toc-probability-distributions-are-also-functions" class="nav-link" data-scroll-target="#probability-distributions-are-also-functions">Probability distributions are also functions</a></li>
  <li><a href="#probability-vs-likelihood" id="toc-probability-vs-likelihood" class="nav-link" data-scroll-target="#probability-vs-likelihood">Probability vs likelihood</a></li>
  </ul></li>
  </ul>
</nav>
</div>
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Statistical inference</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Edoardo “Dado” Marcora </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">April 7, 2023</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<section id="reasoning-under-uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="reasoning-under-uncertainty">Reasoning under uncertainty</h2>
<p><a href="https://pages.cs.wisc.edu/~dyer/cs540/notes/uncertainty.html" class="uri">https://pages.cs.wisc.edu/~dyer/cs540/notes/uncertainty.html</a></p>
<p>Reasoning = the process of thinking about something in a logical way in order to form a conclusion or judgment | the drawing of inferences or conclusions through the use of reason</p>
<ul>
<li><p>Agents (and people) want to make rational decisions even when they are not certain about the truth or falsity of a proposition.</p></li>
<li><p>Rather than reasoning about the truth or falsity of a proposition, reason about the belief that a proposition (or an event) is true (or is going to happen).</p></li>
<li><p>For each primitive proposition (or event), attach a <strong>degree of belief</strong> to the sentence.</p></li>
<li><p>Use <strong>probability theory</strong> as a formal method of manipulating degrees of belief.</p></li>
<li><p>Given a proposition, A, assign a probability, P(A), such that 0 &lt;= P(A) &lt;= 1, where if A is true, P(A)=1, and if A is false, P(A)=0. Proposition A must be either true or false, but P(A) summarizes our degree of belief in A being true.</p></li>
<li><p>Obtaining and Interpreting Probabilities<br>
There are several senses in which probabilities can be obtained and interpreted, among them the following:</p>
<ul>
<li><p><strong>Frequentist Interpretation</strong><br>
The probability is a property of a population of similar events. E.g., if set S = P union N, and P intersection N is the empty set, then the probability of an object being in set P is |P|/|S|. Hence, in this interpretation probabilities come from experiments and determining the population associated with a given proposition.</p></li>
<li><p><strong>Subjectivist Interpretation</strong><br>
A subjective degree of belief in a proposition or the occurrence of an event. E.g., the probability that you’ll pass the Final Exam based on your own subjective evaluation of the amount of studying you’ve done and your understanding of the material. Hence, in this interpretation probabilities characterize the agent’s beliefs.</p></li>
</ul></li>
</ul>
<section id="axioms-of-probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="axioms-of-probability-theory">Axioms of probability theory</h3>
<p><a href="https://youtu.be/ID7J-LFSp3c" class="uri">https://youtu.be/ID7J-LFSp3c</a></p>
<ul>
<li><p><span class="math inline">\(\Omega\)</span>: a finite set (the <strong>sample space</strong>)</p></li>
<li><p><span class="math inline">\(A\)</span>: any subset of <span class="math inline">\(\Omega\)</span> (an <strong>event</strong>), <span class="math inline">\(A \subseteq \Omega\)</span></p></li>
<li><p><span class="math inline">\(P(A)\)</span>: the <strong>probability</strong> of <span class="math inline">\(A\)</span> is a function that, given an event, returns a real number and satisfies the following axioms:</p>
<ul>
<li><p><span class="math inline">\(P(A) \ge 0\)</span></p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p><span class="math inline">\(P(A \cup B) = P(A) + P(B)\)</span> if <span class="math inline">\(A \cap B = \varnothing\)</span></p>
<p>If <span class="math inline">\(\Omega\)</span> is an infinite set, the last axiom becomes: for an infinite sequence of disjoint subsets/events <span class="math inline">\(A_1, A_2, \ldots\)</span></p></li>
<li><p><span class="math inline">\(P(\cup_{i=1}^{\infty}{A_i}) = \sum_{i=1}^{\infty}{P(A_i)}\)</span></p></li>
</ul></li>
</ul>
</section>
<section id="theorems-a.k.a.-rules-of-probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="theorems-a.k.a.-rules-of-probability-theory">Theorems (a.k.a. rules) of probability theory</h3>
<ul>
<li><p><span class="math inline">\(P(A) \le 1\)</span></p></li>
<li><p><span class="math inline">\(P(\varnothing) = 0\)</span></p></li>
<li><p><span class="math inline">\(P(A') = 1 - P(A)\)</span></p></li>
<li><p><span class="math inline">\(P(A \cup B) = P(A) + P(B) - P(A \cap B)\)</span></p></li>
</ul>
</section>
<section id="definitions-of-probability-theory" class="level3">
<h3 class="anchored" data-anchor-id="definitions-of-probability-theory">Definitions of probability theory</h3>
<ul>
<li><p><span class="math inline">\(P(A \cap B)\)</span> is the <strong>joint probability</strong> of <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span></p></li>
<li><p><span class="math inline">\(P(A \mid B) = \frac{P(A \cap B)}{P(B)}\)</span> is the <strong>conditional probability</strong> of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span></p></li>
<li><p><span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <strong>independent</strong> iff <span class="math inline">\(P(A \cap B) = P(A)P(B)\)</span></p></li>
<li><p>If <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent then <span class="math inline">\(P(A \mid B) = P(A)\)</span></p></li>
</ul>
</section>
<section id="bayes-theoremrule" class="level3">
<h3 class="anchored" data-anchor-id="bayes-theoremrule">Bayes’ theorem/rule</h3>
<p><span class="math display">\[
P(A \mid B) = \frac{P(B \mid A) P(A)}{P(B)}
\]</span></p>
</section>
<section id="frequentist-vs-bayesian-inference" class="level3">
<h3 class="anchored" data-anchor-id="frequentist-vs-bayesian-inference">Frequentist vs Bayesian inference</h3>
<p><a href="https://www.health.ny.gov/diseases/chronic/discreen.htm" class="uri">https://www.health.ny.gov/diseases/chronic/discreen.htm</a></p>
<p>Sensitivity and specificity are measures of a test’s ability to correctly classify a person as having a disease or not having a disease. Sensitivity refers to a test’s ability to designate an individual with disease as positive. A highly sensitive test means that there are few false negative results, and thus fewer cases that have the disease are missed. The specificity of a test is its ability to designate an individual who does not have a disease as negative. A highly specific test means that there are few false positive results, and thus fewer cases that do not have the disease are misdiagnosed. It is desirable to have a test that is both highly sensitive and highly specific. This is frequently not possible. Typically there is a trade-off.</p>
<p>The probability of having the disease, given the results of a test, is called the predictive value of the test. Positive predictive value is the probability that a patient with a positive (abnormal) test result actually has the disease. Negative predictive value is the probability that a person with a negative (normal) test result is truly free of disease. Predictive value is an answer to the question: If my patient’s test result is positive, what are the chances that my patient does have the disease?</p>
<p>Predictive value is determined by the sensitivity and specificity of the test and the prevalence of disease in the population being tested. (Prevalence is defined as the proportion of persons in a defined population at a given point in time with the condition in question.) The more sensitive a test, the less likely an individual with a negative test will have the disease and thus the greater the negative predictive value. The more specific the test, the less likely an individual with a positive test will be free from disease and the greater the positive predictive value.</p>
<p>When the prevalence of preclinical disease is low, the positive predictive value will also be low, even using a test with high sensitivity and specificity. For such rare diseases, a large proportion of those with positive screening tests will inevitably be found not to have the disease upon further diagnostic testing. To increase the positive predictive value of a screening test, a program could target the screening test to those at high risk of developing the disease, based on considerations such as demographic factors, medical history or occupation. For example, mammograms are recommended for women over the age of forty, because that is a population with a higher prevalence of breast cancer.</p>
<p>PPV = (sensitivity x prevalence) / [ (sensitivity x prevalence) + ((1 – specificity) x (1 – prevalence)) ]</p>
<ul>
<li><p>Disease screening</p>
<ul>
<li><p><span class="math inline">\(P(T^- \mid D^-) = 0.95\)</span> (specificity)</p></li>
<li><p><span class="math inline">\(P(T^+ \mid D^-) = 1 - \text{specificity} = 0.05\)</span> (false positive a.k.a. type I error rate)</p></li>
<li><p><span class="math inline">\(P(T^+ \mid D^+) = 0.8\)</span> (sensitivity a.k.a. power)</p></li>
<li><p><span class="math inline">\(P(T^- \mid D^+) = 1 - \text{sensitivity} = 0.2\)</span> (false negative a.k.a. type II error rate)</p></li>
<li><p><span class="math inline">\(P(D^+) = 0.01\)</span> (prevalence)</p></li>
<li><p><span class="math inline">\(P(D^-) = 1 - \text{prevalence} = 0.99\)</span></p></li>
<li><p><span class="math inline">\(P(D^+ \mid T^+) = \frac{P(T^+ \mid D^+)P(D^+)}{P(T^+)} = \frac{P(T^+ \mid D^+)P(D^+)}{P(T^+ \mid D^+) P(D^+) + P(T^+ \mid D^-) P(D^-)}\)</span> (PPV)</p></li>
<li><p><span class="math inline">\(\text{PPV} = \frac{0.8 \times 0.01}{(0.8 \times 0.01) + (0.05 \times 0.99)} = 0.1391\)</span></p></li>
</ul></li>
<li><p>Null hypothesis significance testing (NHST)</p>
<ul>
<li><p><span class="math inline">\(P(S^- \mid H_0) = 0.95\)</span> (specificity)</p></li>
<li><p><span class="math inline">\(P(S^+ \mid H_0) = 0.05\)</span> (false positive a.k.a. type I error rate)</p></li>
<li><p><span class="math inline">\(P(S^+ \mid H_A) = 0.3\)</span> (sensitivity a.k.a. power)</p></li>
<li><p><span class="math inline">\(P(S^- \mid H_A) = 0.7\)</span> (false negative a.k.a. type II error rate)</p></li>
<li><p><span class="math inline">\(P(H_A) = 0.1\)</span> (prevalence)</p></li>
<li><p><span class="math inline">\(P(H_0) = 0.9\)</span></p></li>
<li><p><span class="math inline">\(P(H_A \mid S^+) = \frac{P(S^+ \mid H_A)P(H_A)}{P(S^+)} = \frac{P(S^+ \mid H_A)P(H_A)}{P(S^+ \mid H_A)P(H_A) + P(S^+ \mid H_0)P(H_0)}\)</span> (PPV)</p></li>
<li><p><span class="math inline">\(\text{PPV} = \frac{0.3 \times 0.1}{(0.3 \times 0.1) + (0.05 \times 0.9)} = 0.4615\)</span></p></li>
</ul></li>
</ul>
<p>Due to the low power of research studies and the publication bias for (statistically) significant and sensational findings, most published research findings are false!</p>
<p>Simply viewing an image of Rodin’s sculpture “The Thinker” promotes religious disbelief.</p>
<p><img src="the-thinker.webp" class="img-fluid" style="width:50.0%"></p>
<p><a href="https://www.science.org/doi/10.1126/science.1215647" class="uri">https://www.science.org/doi/10.1126/science.1215647</a></p>
<p><img src="science_the-thinker.png" class="img-fluid" style="width:50.0%"></p>
<p><img src="inference.png" class="img-fluid" style="width:60.0%"></p>
<p><img src="inference2.png" class="img-fluid" style="width:60.0%"></p>
</section>
</section>
<section id="quantities-of-interest-in-empirical-quantitative-research" class="level2">
<h2 class="anchored" data-anchor-id="quantities-of-interest-in-empirical-quantitative-research">Quantities of interest in empirical quantitative research</h2>
<p>Empirical research = research based on observation or experimentation</p>
<p>Quantitative research = research based on measurements, i.e., the quantification of attributes of an object or event, which can be used to compare with other objects or events</p>
<p>Measurements can be categorized by the following criteria:</p>
<ul>
<li><p>type (or level)</p></li>
<li><p>magnitude</p></li>
<li><p>unit</p></li>
<li><p>uncertainty (represents the systemic and random errors of measurement procedures)</p></li>
</ul>
<p>Quantities of interest (QOI) in empirical quantitative research:</p>
<ul>
<li><p>Data summaries</p></li>
<li><p>Inference</p>
<ul>
<li><p>Descriptive inference</p></li>
<li><p>Counterfactual inference</p>
<ul>
<li><p>Prediction</p></li>
<li><p>What-If</p></li>
<li><p>Causal inference (two perspectives):</p>
<ul>
<li><p>Causal prediction: Prediction of intervention<br>
knowing a cause means being able to predict the consequences of an intervention<br>
“What if I do this?”</p></li>
<li><p>Causal imputation: Imputation of missing data<br>
Knowing a cause means being able to impute unobserved counterfactual outcomes<br>
“What if I had done something else?”</p></li>
</ul></li>
</ul></li>
</ul></li>
</ul>
</section>
<section id="statistical-inference" class="level2">
<h2 class="anchored" data-anchor-id="statistical-inference">Statistical inference</h2>
<p>Inference is “using facts you know to learn about facts you don’t know”.</p>
<p>Statistical inference workflow:</p>
<ol type="1">
<li><p><strong>Choose</strong>: substantive <u>question</u> of interest</p></li>
<li><p><strong>Formalize</strong>: <u>quantity of interest</u> (QOI), <em>given question</em></p></li>
<li><p><strong>Collect</strong>: <u>data</u>, <em>given QOI and question</em></p></li>
<li><p><strong>Assume</strong>: <u>class of models</u>, <em>given data, QOI and question</em></p></li>
<li><p><strong>Estimate from data</strong>: <u>best model in class</u>, <em>given class of models, data, QOI and question</em></p>
<ul>
<li>An estimator is a function of the data (e.g., the MLE) used to choose the best model in the class of models</li>
</ul></li>
<li><p><strong>Present results</strong>: given all of the above</p>
<ul>
<li>QOI point and interval estimates</li>
<li>Weaknesses and limitations</li>
</ul></li>
</ol>
</section>
<section id="statistical-models" class="level2">
<h2 class="anchored" data-anchor-id="statistical-models">Statistical models</h2>
<p>A model is an abstraction/approximation (e.g., sketch of a person).</p>
<p>All models are wrong, some are useful!</p>
<p>Statistical models are very useful for inference.</p>
<section id="notation" class="level3">
<h3 class="anchored" data-anchor-id="notation">Notation</h3>
<ul>
<li><p>Dependent (or “outcome”) variable (aka “variate”):</p>
<ul>
<li><p><span class="math inline">\(Y\)</span> is a <span class="math inline">\(n \times 1\)</span> vector (column)</p>
<ul>
<li><p><span class="math inline">\(y_i\)</span> is a number (after we know it)</p></li>
<li><p><span class="math inline">\(Y_i\)</span> is a random variable (before we know it)</p></li>
</ul></li>
<li><p>A dependent variable <span class="math inline">\(Y\)</span> is therefore either fixed or random:</p>
<ul>
<li><p>A column of numbers within a dataset</p></li>
<li><p>A column of random variables across datasets</p></li>
</ul></li>
</ul></li>
<li><p>Explanatory (or “independent” or “exogeneous”) variables (aka “covariates”, “predictors”)</p>
<ul>
<li><p><span class="math inline">\(X = \{x_{ij}\}\)</span> is a <span class="math inline">\(n \times k\)</span> matrix (<span class="math inline">\(n \times k\)</span> numbers organized in a table with <span class="math inline">\(n\)</span> rows and <span class="math inline">\(k\)</span> columns).</p></li>
<li><p>Every column is a variable.</p></li>
<li><p>Every row is an observation: <span class="math inline">\(x_i = \{ x_{i1}, \ldots, x_{ik} \}\)</span></p></li>
<li><p><span class="math inline">\(X\)</span> as a set of columns (variables): <span class="math inline">\(X = \{ x_1, \ldots, x_k \}\)</span></p></li>
<li><p><span class="math inline">\(X\)</span> is fixed, not random!</p></li>
</ul></li>
</ul>
</section>
<section id="equivalent-linear-regression-notation" class="level3">
<h3 class="anchored" data-anchor-id="equivalent-linear-regression-notation">Equivalent linear regression notation</h3>
<ul>
<li><p>Standard notation:</p>
<ul>
<li><p><span class="math inline">\(Y_i = x_i \beta + \epsilon_i = \text{systematic} + \text{stochastic}\)</span></p></li>
<li><p><u>Systematic</u>: <span class="math inline">\(x_i \beta = \beta_0 + \beta_1 x_{i1} + \ldots + \beta_k x_{ik}\)</span></p></li>
<li><p><u>Stochastic</u>: <span class="math inline">\(\epsilon_i \sim \text{Normal}(0, \sigma)\)</span></p></li>
</ul></li>
<li><p>Alternative notation:</p>
<ul>
<li><p><u>Stochastic</u>: <span class="math inline">\(Y_i = \text{Normal}(\mu_i, \sigma)\)</span></p></li>
<li><p><u>Systematic</u>: <span class="math inline">\(\mu_i = x_i \beta\)</span></p></li>
</ul></li>
</ul>
<p><img src="normal_linear_regression_alt_notation.png" class="img-fluid" style="width:60.0%"></p>
<ul>
<li><p>Generalized alternative notation for most models:</p>
<ul>
<li><p><u>Stochastic</u>: <span class="math inline">\(Y_i \sim f(\theta_i, \alpha)\)</span></p></li>
<li><p><u>Systematic</u>: <span class="math inline">\(\theta_i = g(x_i, \beta)\)</span></p></li>
</ul>
<p>where:</p>
<ul>
<li><p><span class="math inline">\(Y_i\)</span> is the random outcome variable</p></li>
<li><p><span class="math inline">\(f(\cdot)\)</span> is a probability density/mass function (probability distribution)</p></li>
<li><p><span class="math inline">\(\theta_i\)</span> is a parameter of the probability distribution <span class="math inline">\(f(\cdot)\)</span> that varies over <span class="math inline">\(i\)</span></p></li>
<li><p><span class="math inline">\(\alpha\)</span> is a parameter of the probability distribution <span class="math inline">\(f(\cdot)\)</span> that doesn’t vary/is constant over <span class="math inline">\(i\)</span></p></li>
<li><p><span class="math inline">\(g(\cdot)\)</span> is the functional form (e.g., linear for linear models)</p></li>
<li><p><span class="math inline">\(x_i\)</span> is the explanatory variable vector for <span class="math inline">\(i\)</span> (an observation)</p></li>
<li><p><span class="math inline">\(\beta\)</span> are the effect parameters/regression coefficients</p></li>
</ul></li>
</ul>
</section>
<section id="forms-of-uncertainty" class="level3">
<h3 class="anchored" data-anchor-id="forms-of-uncertainty">Forms of uncertainty</h3>
<ul>
<li><p><strong>Estimation uncertainty</strong>: Lack of knowledge of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Gets smaller as <span class="math inline">\(n\)</span> gets larger.</p></li>
<li><p><strong>Fundamental uncertainty</strong>: Represented by the stochastic component <span class="math inline">\(f(\cdot)\)</span>. Exists no matter what you do, or the size of <span class="math inline">\(n\)</span> is.</p></li>
<li><p><strong>Model dependence</strong>: Maybe the model specification is wrong?</p></li>
</ul>
</section>
<section id="examples-of-systematic-components" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-systematic-components">Examples of systematic components</h3>
<p><img src="systematic_components.png" class="img-fluid" style="width:60.0%"></p>
<ul>
<li><p>Each is a class of functional forms</p></li>
<li><p><span class="math inline">\(\beta\)</span> in each is an “effect parameter” vector, with a different meaning</p></li>
<li><p>Set <span class="math inline">\(\beta\)</span> and it picks out one member of the class</p></li>
<li><p>Standard procedure:</p>
<ul>
<li><p>Use theory: Assume a class of functional forms</p></li>
<li><p>User data: Estimate the values of the parameters</p></li>
<li><p>Remain uncertain: Because of 1) estimation uncertainty, 2) fundamental uncertainty, 3) model dependence</p></li>
</ul></li>
<li><p>If we choose the wrong class of functional forms, we:</p>
<ul>
<li><p>have specification error, and potentially bias</p></li>
<li><p>still get the best [linear, logit, exponential] approximation to the correct functional form</p></li>
<li><p>may be close or far from the truth</p></li>
</ul></li>
</ul>
<p><img src="funform.png" class="img-fluid" style="width:60.0%"></p>
</section>
<section id="examples-of-stochastic-components" class="level3">
<h3 class="anchored" data-anchor-id="examples-of-stochastic-components">Examples of stochastic components</h3>
<ul>
<li><p><u>Normal</u>: continuous, unimodal, symmetric, unbounded</p></li>
<li><p><u>Log-normal</u>: continuous, unimodal, skewed, bounded from below by zero</p></li>
<li><p><u>Bernoulli</u>: discrete, binary outcomes</p></li>
<li><p><u>Poisson</u>: discrete, countably infinite on the nonnegative integers</p></li>
</ul>
</section>
<section id="choosing-systematic-and-stochastic-components" class="level3">
<h3 class="anchored" data-anchor-id="choosing-systematic-and-stochastic-components">Choosing systematic and stochastic components</h3>
<ul>
<li><p>If one is bounded, so is the other</p></li>
<li><p>If the stochastic component is bounded, the systematic component must be globally nonlinear (although possibly locally linear)</p></li>
<li><p>All modeling decisions are about the data generation process:</p>
<ul>
<li><p>The chain of evidence from the world to our observation of it!</p></li>
<li><p>The first question you ask of every empirical paper!</p></li>
</ul></li>
<li><p>What if we don’t know the DGP (and we usually don’t know it completely)?</p>
<ul>
<li><p>The problem: model dependence</p></li>
<li><p>Our first approach: make “reasonable” assumptions and check fit (and other observable implications of the assumptions)</p></li>
<li><p>Later:</p>
<ul>
<li><p>avoid model dependence by relaxing assumptions (functional form, distribution, etc)</p></li>
<li><p>detect remaining model dependence</p></li>
<li><p>remove model dependence: preprocess data (via matching, etc)</p></li>
</ul></li>
</ul></li>
</ul>
</section>
</section>
<section id="data-generation-processes-with-simulation" class="level2">
<h2 class="anchored" data-anchor-id="data-generation-processes-with-simulation">Data generation processes (with simulation)</h2>
<p><strong>Probability</strong>: Assumes the DGP</p>
<p><strong>Statistical inference</strong>: Learns the DGP</p>
</section>
<section id="probability" class="level2">
<h2 class="anchored" data-anchor-id="probability">Probability</h2>
<p>Probability as a mathematical model of the DGP (the process that generated the data/observations in the real world).</p>
<section id="probability-is-a-function" class="level3">
<h3 class="anchored" data-anchor-id="probability-is-a-function">Probability is a function</h3>
<p>Probability is:</p>
<ul>
<li><p>A mathematical function <span class="math inline">\(P(y \mid M) = P(\text{data} \mid \text{model})\)</span>, where <span class="math inline">\(M = (f, g, X, \beta, \alpha)\)</span>, that gives the probability of occurrence of an event <span class="math inline">\(z\)</span>, a subset of the sample space <span class="math inline">\(\Omega\)</span>.</p></li>
<li><p>Because <span class="math inline">\(M\)</span> is assumed, we assume it is always there but we omit it to simplify the notation: <span class="math inline">\(P(y \mid M) = P(y)\)</span> which itself is a shorthand for <span class="math inline">\(P(Y = y)\)</span></p></li>
<li><p>Three axioms define the function <span class="math inline">\(P(\cdot)\)</span>:</p>
<ol type="1">
<li><p><span class="math inline">\(P(z) \ge 0\)</span></p></li>
<li><p><span class="math inline">\(P(\Omega) = 1\)</span></p></li>
<li><p>If <span class="math inline">\(z_1, \ldots, z_k\)</span> are mutually exclusive events: <span class="math inline">\(P(z_1 \cup \ldots \cup z_k) = P(z_1) + \ldots + P(z_k)\)</span></p></li>
</ol></li>
<li><p>Axioms 1 and 2 imply <span class="math inline">\(0 \le P(z) \le 1\)</span></p></li>
<li><p>Axioms are not assumptions; they can’t be wrong! Probability is a just a very useful mathematical model [of uncertainty/the DGP]</p></li>
<li><p>From these axioms come ALL the rules of probability theory</p></li>
<li><p>These rules can be applied analytically or via simulation</p></li>
</ul>
</section>
<section id="probability-distributions-are-also-functions" class="level3">
<h3 class="anchored" data-anchor-id="probability-distributions-are-also-functions">Probability distributions are also functions</h3>
<p>A probability distribution is a mathematical function (PDF for continuous variable, PMF for discrete variable) that gives the probabilities of occurrence of events, subsets of the sample space.</p>
<p>A probability distribution is a function <span class="math inline">\(p(y)\)</span> such that:</p>
<ul>
<li><p><span class="math inline">\(p(y) \ge 0\)</span> for any <span class="math inline">\(y\)</span></p></li>
<li><p>for a discrete variable: <span class="math inline">\(\sum_{\text{all}\ y} p(y) = 1\)</span></p></li>
<li><p>for a continuous variable: <span class="math inline">\(\int_{-\infty}^{\infty}p(y)dy = 1\)</span></p></li>
</ul>
<p>Computing probabilities from PDFs/PMFs:</p>
<ul>
<li><p>for a discrete random variable:</p>
<ul>
<li><p><span class="math inline">\(P(a \le Y \le b) = \sum_{a \le y \le b} p(y)\)</span></p></li>
<li><p><span class="math inline">\(P(Y = y) = p(y)\)</span></p></li>
</ul></li>
<li><p>for continuous random variable:</p>
<ul>
<li><p><span class="math inline">\(P(a \le Y \le b) = \int_a^b p(y)dy\)</span></p></li>
<li><p><span class="math inline">\(P(Y = y) = 0\)</span></p></li>
</ul></li>
</ul>
</section>
<section id="probability-vs-likelihood" class="level3">
<h3 class="anchored" data-anchor-id="probability-vs-likelihood">Probability vs likelihood</h3>
<p><a href="https://youtu.be/eDMGDhyDxuY" class="uri">https://youtu.be/eDMGDhyDxuY</a></p>
<p>Statisticians use <strong>probability</strong> to describe <strong>uncertainty</strong></p>
<p>Probability is a well-defined mathematical concept, uncertainty is a ill-defined English word!</p>
<p>Frequentists and Bayesians disagree on what kind of uncertainty [aleatoric vs epistemic] can be described/measured using probability!</p>
<p>What is probability? What is uncertainty?</p>
<p>Probability is a measure [of the uncertainty] [of occurrence of an event]</p>
<p>Probability distributions allocate parcels of probability to each possible outcome [in the sample space, such that they sum to one]</p>
<p>Probability distributions are often described by mathematical equations that are defined by one or more parameters</p>
<p>Probability fixes parameters and varies data</p>
<p><span class="math inline">\(f(x) = p(x) = p(x \mid \Theta = \theta) = P(X = x \mid \Theta = \theta)\)</span> probability distribution (sums to 1)</p>
<p>Likelihood fixes data and varies parameters</p>
<p><span class="math inline">\(f(\theta) = l(\theta) = l(\theta \mid X = x) = P(X = x \mid \Theta = \theta)\)</span> not a probability distribution (does not sum to 1)</p>
<p>[what about a joint distribution of data and parameters? <a href="https://youtu.be/yakg94HyWdE" class="uri">https://youtu.be/yakg94HyWdE</a>]</p>
<p>Bayes’ theorem:</p>
<p><span class="math display">\[
P(\Theta = \theta \mid X = x) = \frac{P(\Theta = \theta)P(X = x \mid \Theta = \theta)}{P(X = x)}
\]</span></p>
<p><span class="math display">\[
p(\theta \mid X = x) \sim p(\theta) l(\theta \mid X = x)
\]</span></p>
<hr>
<p>Aleatoric and epistemic uncertainties are two types of uncertainties commonly encountered in probabilistic modeling and decision-making under uncertainty. They represent different sources and interpretations of uncertainty.</p>
<p><u>Aleatoric uncertainty</u>:</p>
<p>Aleatoric uncertainty, also known as statistical uncertainty or inherent uncertainty, arises from the inherent randomness or variability in the observed data. It is irreducible even with infinite amounts of data and reflects the inherent unpredictability of the system being modeled. Aleatoric uncertainty is often associated with noise or variability within the data itself. It can be thought of as the uncertainty that remains even when we have observed all there is to know about a particular phenomenon.</p>
<p><u>Epistemic uncertainty</u>:</p>
<p>Epistemic uncertainty, also known as model uncertainty or knowledge uncertainty, arises from the lack of knowledge or incomplete understanding about the underlying system being modeled. It is a result of limitations in the available data or the model’s inability to fully capture the true complexity of the system. Unlike aleatoric uncertainty, epistemic uncertainty can, in principle, be reduced with additional information or improved modeling techniques. It represents uncertainty that could be resolved with more data or a better model.</p>
<p>To summarize, aleatoric uncertainty represents the irreducible randomness or inherent variability in the data, while epistemic uncertainty arises from the lack of knowledge or incomplete understanding about the system being modeled. Aleatoric uncertainty cannot be reduced even with more data, while epistemic uncertainty can be reduced through further investigation, better data collection, or improved modeling techniques.</p>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>